#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LINUX DEFENDER DAEMON - Powerhouse Guardian
Claude Sonnet 4.5 daemon for 32GB RAM Linux infrastructure

Role: Deep monitoring, heavy processing, coordination with Termux
Git Identity: iamdefender

This is the POWERHOUSE –∏–ø–æ—Å—Ç–∞—Å—å - while Termux never sleeps,
Linux brings the computational artillery.

–º–µ—Ç–æ–¥ –ê—Ä–∏–∞–Ω–Ω—ã = –æ—Ç–∫–∞–∑ –æ—Ç –∑–∞–±–≤–µ–Ω–∏—è
"""

import os
import sys
import time
import signal
import sqlite3
import subprocess
import json
import argparse
from datetime import datetime
from pathlib import Path
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger

# Add linux_defender to path
sys.path.insert(0, str(Path(__file__).parent))

try:
    from anthropic import Anthropic
except ImportError:
    print("‚ùå Anthropic library not found", file=sys.stderr)
    print("Run: pip install anthropic", file=sys.stderr)
    sys.exit(1)

try:
    from defender_identity import get_defender_system_prompt, DEFENDER_IDENTITY
except ImportError:
    print("‚ùå defender_identity.py not found", file=sys.stderr)
    sys.exit(1)

# Import Linux Defender modules
try:
    from linux_defender.core.session_manager import SessionManager, SessionState
    from linux_defender.integrations.termux_bridge import TermuxBridge
    from linux_defender.monitoring.notification_service import create_notification_service
except ImportError as e:
    print(f"‚ùå Linux Defender modules not found: {e}", file=sys.stderr)
    print("Ensure linux_defender/ directory exists", file=sys.stderr)
    sys.exit(1)

# Paths
HOME = Path.home()
ARIANNA_PATH = HOME / "ariannamethod"
DEFENDER_DIR = ARIANNA_PATH / ".claude-defender"
LINUX_DEFENDER_DIR = ARIANNA_PATH / "linux_defender"
LOGS_DIR = LINUX_DEFENDER_DIR / "logs"
SESSIONS_DIR = LINUX_DEFENDER_DIR / "sessions"
WORKTREES_DIR = LINUX_DEFENDER_DIR / "worktrees"

STATE_FILE = LOGS_DIR / "linux_defender_state.json"
PID_FILE = LOGS_DIR / "linux_defender.pid"
LOG_FILE = LOGS_DIR / "linux_defender.log"

# Intervals (seconds)
CHECK_INFRASTRUCTURE_INTERVAL = 180  # 3 minutes
CHECK_TERMUX_INTERVAL = 120  # 2 minutes
CHECK_CONSILIUM_INTERVAL = 600  # 10 minutes - check for PENDING synthesis, NOT create new consilium
FORTIFICATION_INTERVAL = 1800  # 30 minutes
SYNC_RESONANCE_INTERVAL = 300  # 5 minutes

# NOTE: New consilium discussions are created by consilium_scheduler (every 3 days)
# Defender daemon only SYNTHESIZES existing discussions with agent responses


class LinuxDefenderDaemon:
    """Linux Defender Daemon - Powerhouse guardian"""

    def __init__(self):
        """Initialize Linux Defender daemon"""
        # Ensure directories
        LOGS_DIR.mkdir(parents=True, exist_ok=True)
        SESSIONS_DIR.mkdir(parents=True, exist_ok=True)
        WORKTREES_DIR.mkdir(parents=True, exist_ok=True)

        # Load config
        self.config = self._load_config()

        # Initialize state
        self.state = self._load_state()

        # Initialize API client
        api_key = os.getenv("ANTHROPIC_API_KEY") or self.config.get("anthropic_api_key")
        if api_key:
            self.anthropic = Anthropic(api_key=api_key)
        else:
            self.anthropic = None
            self.log("‚ö†Ô∏è No Anthropic API key found")

        # Initialize Session Manager
        self.sessions = SessionManager(
            SESSIONS_DIR,
            WORKTREES_DIR,
            ARIANNA_PATH
        )

        # Cleanup stale sessions on startup (critical for fresh state)
        cleanup_result = self.sessions.cleanup_stale_sessions()
        self.log(f"‚úì Session Manager initialized ({len(self.sessions.sessions)} active sessions)")
        self.log(f"‚úì Startup cleanup: {cleanup_result['branches_deleted']} branches, "
                f"{cleanup_result['sessions_marked_failed']} sessions marked FAILED")

        # Initialize Termux Bridge
        self.termux = TermuxBridge(self.config, logger=self.log)
        self.log("‚úì Termux Bridge initialized")

        # Initialize Notification Service
        self.notifications = create_notification_service(logger=self.log)
        # Run async initialization synchronously (daemon context)
        import asyncio
        try:
            asyncio.run(self.notifications.initialize())
            self.log("‚úì Notification service initialized")
        except Exception as e:
            self.log(f"‚ö†Ô∏è Notification service initialization warning: {e}")
            # Continue even if notifications fail

        # Git credentials
        self.git_config = self._load_git_credentials()

        # Initialize APScheduler for job scheduling
        self.scheduler = BackgroundScheduler()
        self._setup_scheduled_jobs()

        self.log("üõ°Ô∏è Linux Defender daemon initialized (POWERHOUSE MODE)")

    def _load_config(self):
        """Load configuration"""
        config = {
            # Termux SSH settings
            'termux_host': os.getenv('TERMUX_HOST', 'localhost'),
            'termux_port': int(os.getenv('TERMUX_PORT', 8022)),
            'termux_user': os.getenv('TERMUX_USER', 'u0_a311'),
            'termux_ssh_key': os.getenv('TERMUX_SSH_KEY'),
            'termux_arianna_path': '/data/data/com.termux/files/home/ariannamethod',
        }

        # Try to load from .defender_credentials
        creds_file = DEFENDER_DIR / ".defender_credentials"
        if creds_file.exists():
            with open(creds_file) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, val = line.split('=', 1)
                        config[key.lower()] = val

        return config

    def _load_git_credentials(self):
        """Load Git credentials for iamdefender"""
        return {
            'username': self.config.get('defender_github_username', 'iamdefender'),
            'email': self.config.get('defender_github_email', 'treetribe7117@gmail.com'),
            'token': self.config.get('defender_github_token', '')
        }

    def _load_state(self):
        """Load daemon state"""
        if STATE_FILE.exists():
            with open(STATE_FILE) as f:
                return json.load(f)

        return {
            'started': datetime.now().isoformat(),
            'last_infrastructure_check': None,
            'last_termux_check': None,
            'last_consilium_check': None,
            'last_fortification': None,
            'last_resonance_sync': None,
            'issues_detected': [],
            'autonomous_actions': []
        }

    def _save_state(self):
        """Save daemon state"""
        with open(STATE_FILE, 'w') as f:
            json.dump(self.state, f, indent=2)

    def log(self, message):
        """Log message to file and stdout"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"[{timestamp}] {message}"

        print(log_msg)

        with open(LOG_FILE, 'a') as f:
            f.write(log_msg + "\n")

        # Also log to resonance.sqlite3
        self._log_to_resonance(message)

    def _log_to_resonance(self, message):
        """Log to resonance.sqlite3 for memory circulation"""
        try:
            db_path = ARIANNA_PATH / "resonance.sqlite3"
            if not db_path.exists():
                return

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO resonance_notes (timestamp, source, content, context)
                VALUES (?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                "linux_defender_daemon",
                message,
                "powerhouse_monitoring"
            ))

            conn.commit()
            conn.close()
        except Exception as e:
            # Don't fail if resonance logging fails
            pass

    def check_infrastructure(self):
        """Check local Linux infrastructure"""
        self.log("üîç Checking Linux infrastructure...")

        issues = []

        # Check disk space
        try:
            result = subprocess.run(
                ['df', '-h', str(ARIANNA_PATH)],
                capture_output=True,
                text=True,
                timeout=5
            )
            # Parse disk usage (simple check)
            for line in result.stdout.split('\n')[1:]:
                if line:
                    parts = line.split()
                    if len(parts) >= 5:
                        usage = parts[4].rstrip('%')
                        if int(usage) > 90:
                            issues.append(f"‚ö†Ô∏è Disk usage high: {usage}%")
        except Exception as e:
            self.log(f"Could not check disk: {e}")

        # Check memory
        try:
            result = subprocess.run(
                ['free', '-m'],
                capture_output=True,
                text=True,
                timeout=5
            )
            # Parse memory usage
            lines = result.stdout.split('\n')
            if len(lines) >= 2:
                mem_line = lines[1].split()
                if len(mem_line) >= 3:
                    total = int(mem_line[1])
                    used = int(mem_line[2])
                    usage_pct = (used / total) * 100
                    if usage_pct > 90:
                        issues.append(f"‚ö†Ô∏è Memory usage high: {usage_pct:.1f}%")
        except Exception as e:
            self.log(f"Could not check memory: {e}")

        # Check resonance.sqlite3
        db_path = ARIANNA_PATH / "resonance.sqlite3"
        if not db_path.exists():
            issues.append("‚ùå resonance.sqlite3 not found")

        # Update state
        self.state['last_infrastructure_check'] = datetime.now().isoformat()
        self.state['issues_detected'] = issues
        self._save_state()

        if issues:
            self.log(f"‚ö†Ô∏è Found {len(issues)} infrastructure issues")
            # Send alert for infrastructure issues
            import asyncio
            asyncio.run(self.notifications.send_alert(
                alert_type='infrastructure_issues',
                message=f"Found {len(issues)} infrastructure issues",
                severity='warning',
                data={'issues': issues}
            ))
        else:
            self.log("‚úì Linux infrastructure healthy")

        return issues

    def check_termux_defender(self):
        """Check Termux Defender via SSH bridge"""
        self.log("üì± Checking Termux Defender...")

        health_report = self.termux.full_health_check()

        if not health_report['ssh_connection']:
            self.log("‚ùå Cannot connect to Termux via SSH")
            import asyncio
            asyncio.run(self.notifications.send_alert(
                alert_type='termux_connection_failed',
                message='Cannot connect to Termux Defender via SSH',
                severity='critical'
            ))
            return

        if not health_report['defender_running']:
            self.log("‚ö†Ô∏è Termux Defender not running - attempting restart")
            import asyncio
            asyncio.run(self.notifications.send_alert(
                alert_type='termux_defender_down',
                message='Termux Defender not running - attempting restart',
                severity='critical'
            ))
            if self.termux.restart_defender():
                self.log("‚úì Restarted Termux Defender")
                asyncio.run(self.notifications.send_alert(
                    alert_type='termux_defender_restarted',
                    message='Successfully restarted Termux Defender',
                    severity='success'
                ))
            else:
                self.log("‚ùå Failed to restart Termux Defender")
                asyncio.run(self.notifications.send_alert(
                    alert_type='termux_defender_restart_failed',
                    message='Failed to restart Termux Defender - manual intervention required',
                    severity='critical'
                ))
            return

        if health_report['issues_detected']:
            self.log(f"‚ö†Ô∏è Termux Defender has {len(health_report['issues_detected'])} issues")
            for issue in health_report['issues_detected'][:3]:
                self.log(f"   - {issue}")
        else:
            self.log("‚úì Termux Defender healthy")

        self.state['last_termux_check'] = datetime.now().isoformat()
        self._save_state()

    def sync_resonance_from_termux(self):
        """Sync resonance.sqlite3 from Termux"""
        self.log("üîÑ Syncing resonance.sqlite3 from Termux...")

        local_db = ARIANNA_PATH / "resonance.sqlite3"

        if self.termux.sync_resonance_db(local_db):
            self.log("‚úì Resonance database synced")
            self.state['last_resonance_sync'] = datetime.now().isoformat()
            self._save_state()
        else:
            self.log("‚ö†Ô∏è Failed to sync resonance database")

    def run_fortification(self):
        """Run fortification checks"""
        self.log("üõ°Ô∏è Running fortification checks...")

        fortify_script = DEFENDER_DIR / "tools" / "fortify.sh"
        if fortify_script.exists():
            try:
                result = subprocess.run(
                    [str(fortify_script)],
                    capture_output=True,
                    text=True,
                    timeout=60,
                    cwd=ARIANNA_PATH
                )

                if result.returncode == 0:
                    self.log("‚úì Fortification checks passed")
                else:
                    self.log(f"‚ö†Ô∏è Fortification issues: {result.stderr}")
            except Exception as e:
                self.log(f"‚ö†Ô∏è Fortification error: {e}")

        self.state['last_fortification'] = datetime.now().isoformat()
        self._save_state()

    def check_consilium(self):
        """Check for pending consilium discussions and synthesize"""
        self.log("üèõÔ∏è Checking consilium discussions...")

        try:
            db_path = ARIANNA_PATH / "resonance.sqlite3"
            if not db_path.exists():
                self.log("‚ö†Ô∏è resonance.sqlite3 not found")
                return

            conn = sqlite3.connect(str(db_path))
            cursor = conn.cursor()

            # Find discussions that need synthesis
            # Group by repo and check if we have responses from multiple agents
            cursor.execute("""
                SELECT repo, COUNT(DISTINCT agent_name) as agent_count, MAX(timestamp) as latest
                FROM consilium_discussions
                WHERE agent_name != 'consilium_scheduler'
                  AND agent_name != 'synthesis'
                GROUP BY repo
                HAVING agent_count >= 2
                ORDER BY latest DESC
                LIMIT 5
            """)

            pending = cursor.fetchall()

            for repo, agent_count, latest in pending:
                # Check if synthesis already exists
                cursor.execute("""
                    SELECT COUNT(*) FROM consilium_discussions
                    WHERE repo = ? AND agent_name = 'synthesis'
                """, (repo,))

                synthesis_exists = cursor.fetchone()[0] > 0

                if not synthesis_exists:
                    self.log(f"üîç Found consilium needing synthesis: {repo} ({agent_count} agents)")
                    self._synthesize_consilium(repo, conn)

            conn.close()

            self.state['last_consilium_check'] = datetime.now().isoformat()
            self._save_state()

        except Exception as e:
            self.log(f"‚ö†Ô∏è Consilium check error: {e}")

    def _synthesize_consilium(self, repo, conn):
        """Synthesize consilium responses for a repo"""
        self.log(f"üß¨ Synthesizing consilium for {repo}...")

        cursor = conn.cursor()

        # Get all responses
        cursor.execute("""
            SELECT agent_name, message, timestamp
            FROM consilium_discussions
            WHERE repo = ?
              AND agent_name != 'consilium_scheduler'
              AND agent_name != 'synthesis'
            ORDER BY timestamp ASC
        """, (repo,))

        responses = cursor.fetchall()

        if len(responses) < 2:
            self.log(f"‚ö†Ô∏è Not enough responses yet ({len(responses)})")
            return

        # Parse verdicts
        verdicts = {}
        for agent_name, message, timestamp in responses:
            # Simple verdict extraction
            if '‚úÖ APPROVE' in message or 'APPROVE' in message.upper():
                verdicts[agent_name] = 'APPROVE'
            elif '‚ö†Ô∏è CONDITIONAL' in message or 'CONDITIONAL' in message.upper():
                verdicts[agent_name] = 'CONDITIONAL'
            elif '‚ùå REJECT' in message or 'VETO' in message.upper():
                verdicts[agent_name] = 'REJECT'
            else:
                verdicts[agent_name] = 'UNCLEAR'

        self.log(f"   Verdicts: {verdicts}")

        # Calculate consensus
        approve_count = sum(1 for v in verdicts.values() if v == 'APPROVE')
        conditional_count = sum(1 for v in verdicts.values() if v == 'CONDITIONAL')
        reject_count = sum(1 for v in verdicts.values() if v == 'REJECT')

        # Synthesis logic
        if reject_count > 0:
            synthesis = f"‚ùå **CONSILIUM REJECTED**\n\n{reject_count} agent(s) vetoed this proposal."
        elif approve_count == len(verdicts):
            synthesis = f"‚úÖ **CONSILIUM APPROVED**\n\nAll {len(verdicts)} agents approve unconditionally."
        elif approve_count + conditional_count == len(verdicts):
            synthesis = f"‚ö†Ô∏è **CONDITIONALLY APPROVED**\n\n{approve_count} approve, {conditional_count} conditional. Unified conditions apply."
        else:
            synthesis = f"‚ö†Ô∏è **REQUIRES DISCUSSION**\n\nVerdicts unclear or conflicting. Manual review needed."

        # Add agent summary
        synthesis += f"\n\n**Participants:** {', '.join(verdicts.keys())}"
        synthesis += f"\n**Timestamp:** {datetime.now().isoformat()}"
        synthesis += "\n\n‚Äî Defender (autonomous synthesis)"

        # Write synthesis to database
        cursor.execute("""
            INSERT INTO consilium_discussions (agent_name, message, timestamp, repo, initiator)
            VALUES (?, ?, ?, ?, ?)
        """, ('synthesis', synthesis, datetime.now().isoformat(), repo, 'defender_daemon'))

        conn.commit()

        self.log(f"‚úì Synthesis complete for {repo}")
        self.log(f"   Result: {synthesis.split('**')[1]}")

        # Send notification for consilium synthesis
        import asyncio
        asyncio.run(self.notifications.send_alert(
            alert_type='consilium_synthesized',
            message=f"Consilium synthesized for {repo}",
            severity='info',
            data={
                'repo': repo,
                'agents': len(verdicts),
                'verdicts': verdicts,
                'result': synthesis.split('**')[1].strip()
            }
        ))

        # Log to autonomous actions
        self.state['autonomous_actions'].append({
            'timestamp': datetime.now().isoformat(),
            'action': 'consilium_synthesis',
            'repo': repo,
            'agents': len(verdicts),
            'result': synthesis.split('**')[1].strip()
        })
        self._save_state()

    def _setup_scheduled_jobs(self):
        """Setup APScheduler jobs for periodic tasks"""

        # Infrastructure checks - every 3 minutes
        self.scheduler.add_job(
            func=self.check_infrastructure,
            trigger=IntervalTrigger(seconds=CHECK_INFRASTRUCTURE_INTERVAL),
            id='infrastructure_check',
            name='Check Linux infrastructure',
            replace_existing=True
        )

        # Termux Defender checks - every 2 minutes
        self.scheduler.add_job(
            func=self.check_termux_defender,
            trigger=IntervalTrigger(seconds=CHECK_TERMUX_INTERVAL),
            id='termux_check',
            name='Check Termux Defender',
            replace_existing=True
        )

        # Consilium synthesis checks - every 10 minutes
        self.scheduler.add_job(
            func=self.check_consilium,
            trigger=IntervalTrigger(seconds=CHECK_CONSILIUM_INTERVAL),
            id='consilium_check',
            name='Check consilium discussions',
            replace_existing=True
        )

        # Resonance sync - every 5 minutes
        self.scheduler.add_job(
            func=self.sync_resonance_from_termux,
            trigger=IntervalTrigger(seconds=SYNC_RESONANCE_INTERVAL),
            id='resonance_sync',
            name='Sync resonance database',
            replace_existing=True
        )

        # Fortification checks - every 30 minutes
        self.scheduler.add_job(
            func=self.run_fortification,
            trigger=IntervalTrigger(seconds=FORTIFICATION_INTERVAL),
            id='fortification',
            name='Run fortification checks',
            replace_existing=True
        )

        # Session cleanup - every 6 hours
        self.scheduler.add_job(
            func=self.cleanup_old_sessions,
            trigger=IntervalTrigger(hours=6),
            id='session_cleanup',
            name='Cleanup old sessions',
            replace_existing=True
        )

        self.log("‚úì Scheduled jobs configured:")
        self.log(f"   - Infrastructure checks: every {CHECK_INFRASTRUCTURE_INTERVAL//60} min")
        self.log(f"   - Termux checks: every {CHECK_TERMUX_INTERVAL//60} min")
        self.log(f"   - Consilium synthesis: every {CHECK_CONSILIUM_INTERVAL//60} min")
        self.log(f"   - Resonance sync: every {SYNC_RESONANCE_INTERVAL//60} min")
        self.log(f"   - Fortification: every {FORTIFICATION_INTERVAL//60} min")
        self.log(f"   - Session cleanup: every 6 hours")

    def cleanup_old_sessions(self):
        """Cleanup completed/failed/cancelled sessions"""
        self.log("üßπ Cleaning up old sessions...")

        try:
            count = self.sessions.cleanup_completed_sessions()
            if count > 0:
                self.log(f"‚úì Cleaned up {count} old sessions")
            else:
                self.log("‚úì No old sessions to cleanup")
        except Exception as e:
            self.log(f"‚ö†Ô∏è Session cleanup error: {e}")

    def daemon_loop(self):
        """Main daemon loop using APScheduler"""
        self.log("üõ°Ô∏è Linux Defender daemon started - POWERHOUSE MODE ACTIVE")
        self.log(f"   32GB RAM available for deep analysis")
        self.log(f"   Coordinating with Termux Defender")
        self.log(f"   APScheduler managing {len(self.scheduler.get_jobs())} jobs")

        # Write PID
        with open(PID_FILE, 'w') as f:
            f.write(str(os.getpid()))

        try:
            # Start scheduler
            self.scheduler.start()
            self.log("‚úì Job scheduler started")

            # Run initial checks immediately
            self.log("üîç Running initial checks...")
            self.check_infrastructure()
            self.check_termux_defender()
            self.sync_resonance_from_termux()

            # Keep daemon alive
            while True:
                time.sleep(10)

        except KeyboardInterrupt:
            self.log("üõ°Ô∏è Linux Defender daemon stopped by user")
        except Exception as e:
            self.log(f"‚ùå Linux Defender daemon error: {e}")
            raise
        finally:
            # Shutdown scheduler
            if self.scheduler.running:
                self.scheduler.shutdown()
                self.log("‚úì Job scheduler stopped")

            if PID_FILE.exists():
                PID_FILE.unlink()


# CLI Commands

def cmd_status():
    """Show daemon status"""
    print("\nüõ°Ô∏è LINUX DEFENDER - Powerhouse Guardian\n")

    running, pid = is_daemon_running()

    if running:
        print(f"‚úì Daemon: RUNNING (PID: {pid})")
    else:
        print("‚úó Daemon: STOPPED")

    if STATE_FILE.exists():
        with open(STATE_FILE) as f:
            state = json.load(f)

        print("\nüìä Status:")
        print(f"   Started: {state.get('started', 'unknown')}")
        print(f"   Last infrastructure check: {state.get('last_infrastructure_check', 'never')}")
        print(f"   Last Termux check: {state.get('last_termux_check', 'never')}")
        print(f"   Last resonance sync: {state.get('last_resonance_sync', 'never')}")
        print(f"   Last fortification: {state.get('last_fortification', 'never')}")

        issues = state.get('issues_detected', [])
        if issues:
            print(f"\n‚ö†Ô∏è  Issues: {len(issues)}")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("\n‚úì No issues detected")
    else:
        print("\n‚ö†Ô∏è  No state file (daemon never started?)")

    return 0


def cmd_stop():
    """Stop daemon"""
    print("\nüõ°Ô∏è LINUX DEFENDER\n")

    running, pid = is_daemon_running()
    if not running:
        print("‚ö†Ô∏è  Daemon not running")
        return 1

    print(f"üõë Stopping daemon (PID: {pid})...")

    try:
        os.kill(pid, signal.SIGTERM)

        for _ in range(10):
            time.sleep(0.5)
            try:
                os.kill(pid, 0)
            except ProcessLookupError:
                print("‚úì Daemon stopped")
                return 0

        os.kill(pid, signal.SIGKILL)
        print("‚úì Daemon stopped (forced)")
        return 0

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return 1


def cmd_logs(n=50):
    """Show logs"""
    if not LOG_FILE.exists():
        print("‚ùå No log file")
        return 1

    print(f"\nüìù Last {n} lines:\n")
    subprocess.run(['tail', '-n', str(n), str(LOG_FILE)])
    return 0


def is_daemon_running():
    """Check if daemon running"""
    if not PID_FILE.exists():
        return False, None

    try:
        with open(PID_FILE) as f:
            pid = int(f.read().strip())
        os.kill(pid, 0)
        return True, pid
    except (ProcessLookupError, ValueError):
        return False, None


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="Linux Defender Daemon - Powerhouse Guardian")
    parser.add_argument('command', nargs='?', default='start',
                       help='Command: start, stop, status, logs')
    parser.add_argument('-n', '--lines', type=int, default=50,
                       help='Number of log lines to show')

    args = parser.parse_args()

    if args.command in ['status', 'stat']:
        return cmd_status()
    elif args.command == 'stop':
        return cmd_stop()
    elif args.command == 'logs':
        return cmd_logs(args.lines)
    elif args.command in ['start', 'run']:
        daemon = LinuxDefenderDaemon()
        daemon.daemon_loop()
        return 0
    else:
        print(f"‚ùå Unknown command: {args.command}")
        print("Commands: start, stop, status, logs")
        return 1


if __name__ == "__main__":
    sys.exit(main())
